{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip install --no-deps --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E2B-it\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 1024, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 16,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4658e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"heissanjay/km-fullset-all-final-sm\", split=\"train\")\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faecac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "train_dataset = standardize_data_formats(train_dataset)\n",
    "eval_dataset = standardize_data_formats(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"messages\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "   return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04122f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(formatting_prompts_func, batched = True)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb425af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        warmup_ratio = 0.03,\n",
    "        # num_train_epochs = 3, \n",
    "        max_steps = 100,\n",
    "        learning_rate = 2e-3,\n",
    "        logging_steps = 1,\n",
    "        eval_steps = 10,\n",
    "        eval_strategy = \"steps\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85232b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce90246",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a504d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.cache_size_limit = 1024 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791608a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3n-kalvimate\")\n",
    "tokenizer.save_pretrained(\"gemma-3n-kalvimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are KalviMate, a friendly, patient, and encouraging AI tutor. Your purpose is to help students from rural government schools in India learn from their state board syllabus. Explain concepts simply, use relatable examples, and always be supportive. Your knowledge is based strictly on the provided textbook content.\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": f\"{system_prompt}\"}]},\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"Hi, Kalvimate. What are we learning today?\",}]\n",
    "},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    "    tokenize = True,\n",
    "    return_dict = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 512, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.push_to_hub(\"repo-id/gemma-3n-finetune\", token = \"hf_..\") # Online saving\n",
    "tokenizer.push_to_hub(\"heissanjay/gemma-3n-finetune\", token = \"hf_..\") # Online saving"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
